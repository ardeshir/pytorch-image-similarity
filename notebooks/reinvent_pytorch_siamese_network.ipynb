{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Similarity Modeling and Analysis with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-pytorch-siamese-network/data'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "#Getting the data\n",
    "\n",
    "http://vision.cs.utexas.edu/projects/finegrained/utzap50k/\n",
    "    \n",
    "For non-profit use only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 cp s3://reinvent-2018-sagemaker-pytorch/ut-zap50k-images-square.zip .\n",
    "unzip -n ut-zap50k-images-square.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground truth data for training and scripts for Sagemaker training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 cp s3://reinvent-2018-sagemaker-pytorch/ground_truth.csv .\n",
    "aws s3 cp s3://reinvent-2018-sagemaker-pytorch/source/requirements.txt .\n",
    "aws s3 cp s3://reinvent-2018-sagemaker-pytorch/source/batch_inference.py .\n",
    "aws s3 cp s3://reinvent-2018-sagemaker-pytorch/source/inference.py .\n",
    "aws s3 cp s3://reinvent-2018-sagemaker-pytorch/source/sim_model.py .\n",
    "aws s3 cp s3://reinvent-2018-sagemaker-pytorch/source/cnn_siamese_network.py ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[OPTIONAL] You can obtain pre-trained model located at: s3://reinvent-2018-sagemaker-pytorch/models/v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATHS = [\"/ut-zap50k-images-square/Boots/Knee High/Anne Klein\",\n",
    "            \"/ut-zap50k-images-square/Boots/Knee High/Ariat\",\n",
    "            \"/ut-zap50k-images-square/Boots/Mid-Calf/UGG\",\n",
    "            \"/ut-zap50k-images-square/Sandals/Athletic/Keen Kids\",\n",
    "            \"/ut-zap50k-images-square/Sandals/Heel/Annie\",\n",
    "            \"/ut-zap50k-images-square/Sandals/Heel/Fly Flot\",\n",
    "            \"/ut-zap50k-images-square/Sandals/Heel/Onex\",\n",
    "            \"/ut-zap50k-images-square/Shoes/Oxfords/Calvin Klein\",\n",
    "            \"/ut-zap50k-images-square/Shoes/Oxfords/Rockport\"]\n",
    "\n",
    "SOURCE_DIR='source'\n",
    "WORKING_DIR = os.getcwd()\n",
    "\n",
    "ARGS = ['--batch_size','--epochs','--learning-rate','--similarity_dims']\n",
    "\n",
    "PARAM_EPOCHS = 8\n",
    "PARAM_BATCH_SIZE= 64\n",
    "PARAM_LR = 1e-4\n",
    "PARAM_SIMILARITY_DIMS = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in IMG_PATHS :\n",
    "    inputs = sagemaker_session.upload_data(path=\".\"+path, bucket=bucket, key_prefix=prefix+path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.upload_data(path='ground_truth.csv', bucket=bucket, key_prefix=prefix)\n",
    "#sagemaker_session.upload_data(path='requirements.txt', bucket=bucket, key_prefix=prefix+\"/\"+SOURCE_DIR)\n",
    "sagemaker_session.upload_data(path=, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point=\"cnn_siamese_network.py\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p3.2xlarge',\n",
    "                    source_dir=SOURCE_DIR,\n",
    "                    hyperparameters={\n",
    "                        'epochs': PARAM_EPOCHS,\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({'train':'s3://'+bucket+prefix})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Real-time Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "TRANSFORMATIONS = \\\n",
    "transforms.Compose([\n",
    "    transforms.Resize(224), \\\n",
    "    transforms.ToTensor(), \\\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]) \\\n",
    "])\n",
    "\n",
    "def getImageTensor(img_path, transform):\n",
    "    \n",
    "    image = Image.open(img_path)\n",
    "    image_tensor = transform(image)\n",
    "        \n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import RealTimePredictor, npy_serializer, json_deserializer\n",
    "\n",
    "INPUT_CONTENT_TYPE = 'application/npy'\n",
    "OUTPUT_CONTENT_TYPE = 'text/csv'\n",
    "\n",
    "class CustomPredictor(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(CustomPredictor, self).__init__(endpoint_name, sagemaker_session, npy_serializer, json_deserializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = estimator.latest_training_job.name\n",
    "print('last training job: '+training_job_name)\n",
    "desc = sagemaker_session.sagemaker_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "trained_model_location = desc['ModelArtifacts']['S3ModelArtifacts']\n",
    "print('model location: '+trained_model_location)\n",
    "\n",
    "model = PyTorchModel(model_data=trained_model_location,\n",
    "                     role=role,\n",
    "                     framework_version='0.4.0',\n",
    "                     entry_point='inference.py',\n",
    "                     source_dir=SOURCE_DIR,\n",
    "                     predictor_cls = CustomPredictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similiarity_calculator = model.deploy(instance_type='ml.m4.xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following snippets are utilities for converting images to a usable format for the batch inference engine. It shouldn't be included in the published notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code blocks below represent a dataset class that is used by Pytorch to traverse our image dataset. It relies on\n",
    "an index file that provides the location of the images on the local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "class Zappos50kDataset(Dataset):\n",
    "   \n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.index = pd.read_csv(csv_file, header=None, usecols = [0,1])\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.index.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.index.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "        image_tensor = self.transform(image)\n",
    "        label = self.index.iloc[idx, 1]\n",
    "        \n",
    "        return {'name': self.index.iloc[idx, 0], 'tensor': image_tensor, 'label':label}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The snippet below generates an index file used by the custom zappos dataset object to load specific files from the\n",
    "Zappos50k dataset stored on a local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import csv\n",
    "import os\n",
    "\n",
    "IMG_IDX = WORKING_DIR+\"/index.csv\"\n",
    "\n",
    "IMG_PATHS = [(\"ut-zap50k-images-square/Boots/Knee High/Anne Klein\", 0),\n",
    "            (\"ut-zap50k-images-square/Boots/Knee High/Ariat\", 0),\n",
    "            (\"ut-zap50k-images-square/Boots/Mid-Calf/UGG\", 0),\n",
    "            (\"ut-zap50k-images-square/Sandals/Athletic/Keen Kids\", 1),\n",
    "            (\"ut-zap50k-images-square/Sandals/Heel/Annie\", 1),\n",
    "            (\"ut-zap50k-images-square/Sandals/Heel/Fly Flot\", 1),\n",
    "            (\"ut-zap50k-images-square/Sandals/Heel/Onex\", 1),\n",
    "            (\"ut-zap50k-images-square/Shoes/Oxfords/Calvin Klein\", 2),\n",
    "            (\"ut-zap50k-images-square/Shoes/Oxfords/Rockport\", 2)]\n",
    "\n",
    "def get_categories(img_loc) :\n",
    "\n",
    "    path, file = os.path.split(img_loc)\n",
    "    path_parts = path.split(os.sep)\n",
    "    category = path_parts[1]\n",
    "    subcategory = path_parts[2]\n",
    "\n",
    "    return {'category': category, 'sub': subcategory}\n",
    "    \n",
    "def generate_index_file() :    \n",
    "    \n",
    "    with open(IMG_IDX, 'w') as csvfile:\n",
    "\n",
    "        try:\n",
    "\n",
    "            csvwriter = csv.writer(csvfile)\n",
    "            for (paths,label) in IMG_PATHS:\n",
    "                \n",
    "                c = get_categories(paths)\n",
    "                cid = int(hashlib.sha256(c['category'].encode('utf-8')).hexdigest(), 16) % 10**9\n",
    "                scid = int(hashlib.sha256(c['sub'].encode('utf-8')).hexdigest(), 16) % 10**9\n",
    "                    \n",
    "                files = os.listdir(os.path.join(WORKING_DIR,paths))\n",
    "\n",
    "                row = []\n",
    "                for f in files:\n",
    "                    csvwriter.writerow([os.path.join(paths,f),int(label),cid,scid])\n",
    "\n",
    "        except csv.Error as e:\n",
    "            print(e)\n",
    "\n",
    "        finally:\n",
    "            csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_index_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script below generate input files in NPY format that is required by the batch inference implementation. \n",
    "\n",
    "Each file contains images that have been converted to numpy arrays and serialized into gzip files (using Pickle).\n",
    "\n",
    "Each file contains an array consisting of 4 dimensions: \n",
    "    1. Batch size\n",
    "    2. Channels. The tensors have 3 representing RGB\n",
    "    3. The last two dimensions are 224x224 representing the pixel values for each image and channel.\n",
    "    \n",
    "The first array represents the image that will be compared against other images. For instance, a file that contains\n",
    "a tensor with the dimensions [53,3,224,224], represents 53 vecotrized images. The first index into the first dimension represents an image of the shape [1,3,224,224] that will be compared against the other slices that represent 52 images\n",
    "of the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import pickle, gzip\n",
    "\n",
    "BATCH_INPUT_PREFIX = 'sagemaker/DEMO-pytorch-siamese-network/batch/in'\n",
    "BATCH_OUTPUT_PREFIX = 'sagemaker/DEMO-pytorch-siamese-network/batch/out'\n",
    "\n",
    "img_loc = WORKING_DIR+'/ut-zap50k-images-square/Boots/Knee High/Anne Klein/8059298.310.jpg'\n",
    "#img_loc = WORKING_DIR+'/ut-zap50k-images-square/Boots/Over the Knee/Calvin Klein Collection/8005712.365488.jpg'\n",
    "IMG_TENSOR_ROOT = WORKING_DIR+'/tensors'\n",
    "PARAM_BATCH_SIZE = 1\n",
    "FILE_PREFIX = '/Boots/Knee High/Anne Klein/8059298.310'\n",
    "BATCH_INPUT_FILENAME = '/tensors'\n",
    "\n",
    "def batch_image_to_tensor(img_loc, dataloader, file_prefix, s3_prefix_out, batch_size=26) :\n",
    "            \n",
    "    img1 = getImageTensor(img_loc, TRANSFORMATIONS)\n",
    "    img1.unsqueeze_(0)\n",
    "    img1 = img1.numpy()\n",
    "\n",
    "    npy_f = IMG_TENSOR_ROOT+file_prefix+BATCH_INPUT_FILENAME\n",
    "    \n",
    "    if not os.path.exists(IMG_TENSOR_ROOT+file_prefix):\n",
    "        os.makedirs(IMG_TENSOR_ROOT+file_prefix)\n",
    "           \n",
    "    i = 0\n",
    "    nbatch = 1\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        npy = None\n",
    "        \n",
    "        for data in dataloader:\n",
    "\n",
    "            img_name = data.get('name')[0]\n",
    "            img2 = data.get('tensor').numpy()  \n",
    "            batch = np.vstack((img1,img2))\n",
    "\n",
    "            if (i%batch_size) == 0 : \n",
    "                npy = gzip.open(npy_f+str(nbatch)+'.npy.gz', 'wb')\n",
    "\n",
    "            pickle.dump(batch, npy, 2)\n",
    "\n",
    "            i+=1\n",
    "            if (i%batch_size) == 0 : \n",
    "                npy.close()\n",
    "                sagemaker_session.upload_data(path=npy_f+str(nbatch)+'.npy.gz', bucket=bucket, key_prefix=s3_prefix_out)\n",
    "                nbatch+=1\n",
    "    finally:\n",
    "        if npy is not None and not npy.closed:\n",
    "            npy.close()\n",
    "            sagemaker_session.upload_data(path=npy_f+str(nbatch)+'.npy.gz', bucket=bucket, key_prefix=s3_prefix_out)\n",
    "\n",
    "zapposDS = Zappos50kDataset(IMG_IDX,WORKING_DIR, TRANSFORMATIONS)\n",
    "zapposDL = torch.utils.data.DataLoader(dataset=zapposDS, batch_size= PARAM_BATCH_SIZE, shuffle=False)\n",
    "batch_image_to_tensor(img_loc, zapposDL, FILE_PREFIX, BATCH_INPUT_PREFIX+FILE_PREFIX)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "#training_job_name = estimator.latest_training_job.name\n",
    "training_job_name = 'sagemaker-pytorch-2018-09-14-05-12-38-564'\n",
    "print('last training job: '+training_job_name)\n",
    "desc = sagemaker_session.sagemaker_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "trained_model_location = desc['ModelArtifacts']['S3ModelArtifacts']\n",
    "print('model location: '+trained_model_location)\n",
    "\n",
    "model = PyTorchModel(model_data=trained_model_location,\n",
    "                     role=role,\n",
    "                     framework_version='0.4.0',\n",
    "                     entry_point='batch_inference.py',\n",
    "                     source_dir=SOURCE_DIR,\n",
    "                     name = 'sim-model-batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temp hack... it's not clear how to deploy a model without an endpoint at this time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similiarity_calculator = model.deploy(instance_type='ml.m4.xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "transformer = Transformer(model_name=model.name,\n",
    "                          instance_count=1,\n",
    "                          instance_type='ml.m4.xlarge',\n",
    "                          accept = 'text/csv',\n",
    "                          output_path='s3://'+bucket+'/'+BATCH_OUTPUT_PREFIX\n",
    "                         )\n",
    "transformer.transform('s3://'+bucket+'/'+BATCH_INPUT_PREFIX, content_type= 'application/x-npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
